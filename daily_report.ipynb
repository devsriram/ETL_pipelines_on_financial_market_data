{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ab478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import boto3 as bt\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c744a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter layer\n",
    "\n",
    "# Function to convert CSV to Parquet file\n",
    "def read_csv_to_df(bucket, key, decoding = 'utf-8', sep=','):\n",
    "    csv_obj = bucket.Object(key=key).get().get(\"Body\").read().decode(decoding)\n",
    "    data = StringIO(csv_obj)\n",
    "    df = pd.read_csv(data, delimiter=sep)\n",
    "    return df\n",
    "\n",
    "# Function to store dataframe in S3 bucket\n",
    "def write_df_to_s3(bucket, df, key):\n",
    "    out_buffer = BytesIO()\n",
    "    df.to_parquet(out_buffer, index=False)\n",
    "    bucket_target.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "def return_objects(bucket, arg_date):\n",
    "    # get date from which data should be retrived \n",
    "    min_date = datetime.strptime(arg_date, '%Y-%m-%d').date() - timedelta(days=1)\n",
    "    # get all filenames/keys from the source\n",
    "    objects = [obj.key for obj in bucket.objects.all() if datetime.strptime(obj.key.split('/')[0], src_date_format).date() >= min_date]\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78688fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application layer\n",
    "\n",
    "def extract(bucket, objects):\n",
    "    # read each file and append data to dataframe\n",
    "    df = pd.concat([read_csv_to_df(bucket, obj) for obj in objects], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def transform_aggregated_report(df, columns, arg_date):\n",
    "    # select only mentioned columns\n",
    "    df = df.loc[:,columns]\n",
    "    # remove null values\n",
    "    df.dropna(inplace=True)\n",
    "    # Create a column named opening_price which tells the closing price of a stock(ISIN) each day\n",
    "    df['opening_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('first')\n",
    "    # Create a column named closing_price which tells the closing price of a stock(ISIN) each day\n",
    "    df['closing_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('last')\n",
    "    # Aggegations: Get min and max, total values for a stock each day\n",
    "    df = (df.groupby(['ISIN', 'Date'], as_index=False)\n",
    "                    .agg(opening_price_eur = ('opening_price','min'), closing_price_eur = ('closing_price','min'),\n",
    "                        minimum_price_eur = ('MinPrice','min'), maximum_price_eur = ('MaxPrice','max'),\n",
    "                        daily_traded_volume = ('TradedVolume', 'sum')))\n",
    "    # Percent change between closing price of 2 consecutive days\n",
    "    df['prev_closing_price'] = df.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "    df['change_prev_closing_%'] = (df['closing_price_eur'] - df['prev_closing_price']) / df['prev_closing_price'] * 100\n",
    "    df.drop(columns=['prev_closing_price'], inplace=True)\n",
    "    df = df.round(decimals=2)\n",
    "    df = df[df.Date >= arg_date]\n",
    "    return df\n",
    "\n",
    "def load(bucket, df, trg_key, trg_format):\n",
    "    key = trg_key + '_' + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + '.' + trg_format\n",
    "    write_df_to_s3(bucket, df, key)\n",
    "    return True\n",
    "\n",
    "def etl_aggregated_report(src_bucket, trg_bucket, objects, columns, arg_date, trg_key, trg_format):\n",
    "    df = extract(src_bucket, objects)\n",
    "    df = transform_aggregated_report(df, columns, arg_date)\n",
    "    load(trg_bucket, df, trg_key, trg_format)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d219b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function entrypoint\n",
    "\n",
    "def main():\n",
    "    # Parameters/Configurations\n",
    "    # Later read config\n",
    "    # Set arg_date to get records from arg_date to today's date, Format = YYYY-MM-DD\n",
    "    arg_date = '2022-01-10'\n",
    "    src_date_format = '%Y-%m-%d'\n",
    "    src_bucket = 'deutsche-boerse-xetra-pds'\n",
    "    trg_bucket = 'dev-xetra-1'\n",
    "    columns = ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "    trg_key = 'dev_xetra_report'\n",
    "    trg_format = 'parquet'\n",
    "    \n",
    "    # Init\n",
    "    # Connect to AWS s3 bucket\n",
    "    aws_session = bt.Session(\n",
    "          region_name = 'us-east-1', \n",
    "          aws_access_key_id= 'AKIAT464GZVTJY3VEDBN', \n",
    "          aws_secret_access_key= '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')\n",
    "    s3 = aws_session.resource('s3')\n",
    "    bucket_src = s3.Bucket(src_bucket)\n",
    "    bucket_trg = s3.Bucket(trg_bucket)\n",
    "    \n",
    "    # Run application\n",
    "    objects = return_objects(src_bucket, arg_date)\n",
    "    etl_aggregated_report(bucket_src, bucket_trg, objects, columns, arg_date, trg_key, trg_format)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f08ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m bucket_trg \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mBucket(trg_bucket)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run application\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m objects \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_bucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m etl_aggregated_report(bucket_src, bucket_trg, objects, columns, arg_date, trg_key, trg_format)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mreturn_objects\u001b[1;34m(bucket, arg_date)\u001b[0m\n\u001b[0;32m     19\u001b[0m min_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(arg_date, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdate() \u001b[38;5;241m-\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# get all filenames/keys from the source\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m objects \u001b[38;5;241m=\u001b[39m [obj\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;28;01mif\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mstrptime(obj\u001b[38;5;241m.\u001b[39mkey\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m], src_date_format)\u001b[38;5;241m.\u001b[39mdate() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_date]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'objects'"
     ]
    }
   ],
   "source": [
    "# run\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01510683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading uploaded file in s3 bucket\n",
    "trg_bucket = 'dev-xetra-1'\n",
    "aws_session = bt.Session(\n",
    "      region_name = 'us-east-1', \n",
    "      aws_access_key_id= 'AKIAT464GZVTJY3VEDBN', \n",
    "      aws_secret_access_key= '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')\n",
    "s3 = aws_session.resource('s3')\n",
    "bucket_trg = s3.Bucket(trg_bucket)\n",
    "for obj in bucket_trg.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet_obj = bucket_target.Object(key='dev_xetra_report_20220116_104126.parquet').get().get(\"Body\").read()\n",
    "# tar_data = BytesIO(parquet_obj)\n",
    "# df_report = pd.read_parquet(tar_data)\n",
    "# df_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
